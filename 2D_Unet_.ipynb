{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2D_Unet_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2HcKG7WIys9",
        "outputId": "da3f0dee-6726-4471-f287-4912542c3cc3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLh-rHbDseny"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import nibabel as nib\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBxiMe0BsqvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0d87f1-56aa-49b3-c1b5-8b5bd83a7535"
      },
      "source": [
        "from dice_score import *\n",
        "from knee_init import *\n",
        "from New_network import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxmN_KU_suhu"
      },
      "source": [
        "class UNet_2d(nn.Module):\n",
        "  def __init__(self, input_channels =1, num_classes = 2):\n",
        "    super(UNet_2d, self).__init__()\n",
        "    self.input_channels = input_channels\n",
        "    \n",
        "    ####DOWNSAMPLING####\n",
        "    self.conv1 = Downconv(1, 64)\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size =2)\n",
        "    self.conv2 = Downconv(64,128)\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size =2)\n",
        "    self.conv3 = Downconv(128,256)\n",
        "    self.maxpool3 = nn.MaxPool2d(kernel_size =2)\n",
        "    self.conv4 = Downconv(256,512)\n",
        "    self.maxpool4 = nn.MaxPool2d(kernel_size =2)\n",
        "\n",
        "    self.conv5 = Downconv(512,1024)\n",
        "\n",
        "\n",
        "    ####UPSAMPLINIG####\n",
        "    self.upconv4 = Upconv(1024,512)\n",
        "    self.upconv3 = Upconv(512,256)\n",
        "    self.upconv2 = Upconv(256,128)\n",
        "    self.upconv1 = Upconv(128,64)\n",
        "\n",
        "    ####Final/Output####\n",
        "\n",
        "    self.output = output(64,num_classes)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "     \n",
        "    \n",
        "    conv1 = self.conv1(inputs)\n",
        "    maxpool1 = self.maxpool1(conv1)\n",
        "\n",
        "    conv2 = self.conv2(maxpool1)\n",
        "    maxpool2 = self.maxpool2(conv2)\n",
        "\n",
        "    conv3 = self.conv3(maxpool2)\n",
        "    maxpool3 = self.maxpool3(conv3)\n",
        "\n",
        "    conv4 = self.conv4(maxpool3)\n",
        "    maxpool4 = self.maxpool4(conv4)\n",
        "\n",
        "    center = self.conv5(maxpool4)    # final convolution of encoderpart without maxpool layer\n",
        "      \n",
        "    up4 = self.upconv4(center, conv4)\n",
        "    up3 = self.upconv3(up4, conv3)\n",
        "    up2 = self.upconv2(up3, conv2)\n",
        "    up1 = self.upconv1(up2, conv1)\n",
        "\n",
        "    output = self.output(up1)      # output layer/ final layer \n",
        "\n",
        "    return output\n",
        "  @staticmethod\n",
        "  def apply_argmax_softmax(pred):\n",
        "    log_p = F.softmax(pred, dim=1)\n",
        "\n",
        "    return log_p\n",
        "class UNet_2d(nn.Module):\n",
        "  def __init__(self, input_channels =1, num_classes = 2):\n",
        "    super(UNet_2d, self).__init__()\n",
        "    self.input_channels = input_channels\n",
        "    \n",
        "    ####DOWNSAMPLING####\n",
        "    self.conv1 = Downconv(1, 64)\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size =2)\n",
        "    self.conv2 = Downconv(64,128)\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size =2)\n",
        "    self.conv3 = Downconv(128,256)\n",
        "    self.maxpool3 = nn.MaxPool2d(kernel_size =2)\n",
        "    self.conv4 = Downconv(256,512)\n",
        "    self.maxpool4 = nn.MaxPool2d(kernel_size =2)\n",
        "\n",
        "    self.conv5 = Downconv(512,1024)\n",
        "\n",
        "\n",
        "    ####UPSAMPLINIG####\n",
        "    self.upconv4 = Upconv(1024,512)\n",
        "    self.upconv3 = Upconv(512,256)\n",
        "    self.upconv2 = Upconv(256,128)\n",
        "    self.upconv1 = Upconv(128,64)\n",
        "\n",
        "    ####Final/Output####\n",
        "\n",
        "    self.output = output(64,num_classes)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "     \n",
        "    \n",
        "    conv1 = self.conv1(inputs)\n",
        "    maxpool1 = self.maxpool1(conv1)\n",
        "\n",
        "    conv2 = self.conv2(maxpool1)\n",
        "    maxpool2 = self.maxpool2(conv2)\n",
        "\n",
        "    conv3 = self.conv3(maxpool2)\n",
        "    maxpool3 = self.maxpool3(conv3)\n",
        "\n",
        "    conv4 = self.conv4(maxpool3)\n",
        "    maxpool4 = self.maxpool4(conv4)\n",
        "\n",
        "    center = self.conv5(maxpool4)    # final convolution of encoderpart without maxpool layer\n",
        "      \n",
        "    up4 = self.upconv4(center, conv4)\n",
        "    up3 = self.upconv3(up4, conv3)\n",
        "    up2 = self.upconv2(up3, conv2)\n",
        "    up1 = self.upconv1(up2, conv1)\n",
        "\n",
        "    output = self.output(up1)      # output layer/ final layer \n",
        "\n",
        "    return output\n",
        "  @staticmethod\n",
        "  def apply_argmax_softmax(pred):\n",
        "    log_p = F.softmax(pred, dim=1)\n",
        "\n",
        "    return log_p\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9bxOX7qGbOn"
      },
      "source": [
        "def init_weights(m):   # weights initialized using xavier\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_normal(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant(m.bias, 0.0)\n",
        "\n",
        "def countParameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa6IPsskGcfA"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/new_nii_data' # change the path depending on where you store the dataset C:\\Users\\vinkk\\OneDrive\\Desktop\n",
        "image_1 = 'DU01_image.nii'\n",
        "label_1 = 'DU01_label.nii'\n",
        "image_2 = 'DU02_image.nii'\n",
        "label_2 = 'DU02_label.nii' \n",
        "image_3 = 'DU03_image.nii' \n",
        "label_3 = 'DU03_label.nii' \n",
        "image_4 = 'patient1_left_image.nii' \n",
        "label_4 = 'patient1_left_label.nii' \n",
        "image_5 = 'patient2_left_image.nii' \n",
        "label_5 = 'patient2_left_label.nii' \n",
        "image_6 = 'patient1_right_image.nii' \n",
        "label_6 = 'patient1_right_label.nii'\n",
        "image_7 = 'patient2_right_image.nii' \n",
        "label_7 = 'patient2_right_label.nii'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNi4C7XLGeKl"
      },
      "source": [
        "image_1 = nib.load(os.path.join(data_dir,image_1)).get_fdata()\n",
        "label_1 = nib.load(os.path.join(data_dir,label_1)).get_fdata()\n",
        "image_2 = nib.load(os.path.join(data_dir,image_2)).get_fdata()\n",
        "label_2 = nib.load(os.path.join(data_dir,label_2)).get_fdata()\n",
        "image_3 = nib.load(os.path.join(data_dir,image_3)).get_fdata()\n",
        "label_3 = nib.load(os.path.join(data_dir,label_3)).get_fdata()\n",
        "image_4 = nib.load(os.path.join(data_dir,image_4)).get_fdata()\n",
        "label_4 = nib.load(os.path.join(data_dir,label_4)).get_fdata()\n",
        "image_5 = nib.load(os.path.join(data_dir,image_5)).get_fdata()\n",
        "label_5 = nib.load(os.path.join(data_dir,label_5)).get_fdata()\n",
        "image_6 = nib.load(os.path.join(data_dir,image_6)).get_fdata()\n",
        "label_6 = nib.load(os.path.join(data_dir,label_6)).get_fdata()\n",
        "image_7 = nib.load(os.path.join(data_dir,image_7)).get_fdata()\n",
        "label_7 = nib.load(os.path.join(data_dir,label_7)).get_fdata()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPy8-5QBGgqm"
      },
      "source": [
        "label_1 = (np.array(label_1) > 0).astype(np.int)   #changing the range from 0-255 to 0-1 for labels \n",
        "label_2 = (np.array(label_2) > 0).astype(np.int)\n",
        "label_3 = (np.array(label_3) > 0).astype(np.int)\n",
        "label_4 = (np.array(label_4) > 0).astype(np.int)\n",
        "label_5 = (np.array(label_5) > 0).astype(np.int)\n",
        "label_6 = (np.array(label_6) > 0).astype(np.int)\n",
        "label_7 = (np.array(label_7) > 0).astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYKVRWjCGihl"
      },
      "source": [
        "image_1 = torch.from_numpy(image_1)\n",
        "image_2 = torch.from_numpy(image_2)\n",
        "image_3 = torch.from_numpy(image_3)\n",
        "image_4 = torch.from_numpy(image_4)\n",
        "image_5 = torch.from_numpy(image_5)\n",
        "image_6 = torch.from_numpy(image_6)\n",
        "image_7 = torch.from_numpy(image_7)\n",
        "\n",
        "label_1 = torch.from_numpy(label_1)\n",
        "label_2 = torch.from_numpy(label_2)\n",
        "label_3 = torch.from_numpy(label_3)\n",
        "label_4 = torch.from_numpy(label_4)\n",
        "label_5 = torch.from_numpy(label_5)\n",
        "label_6 = torch.from_numpy(label_6)\n",
        "label_7 = torch.from_numpy(label_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDOkSfGaGkEF"
      },
      "source": [
        "image_1 = image_1.unsqueeze(0).permute(3,0,1,2)\n",
        "image_2 = image_2.unsqueeze(0).permute(3,0,1,2)\n",
        "image_3 = image_3.unsqueeze(0).permute(3,0,1,2)\n",
        "image_4 = image_4.unsqueeze(0).permute(3,0,1,2)\n",
        "image_5 = image_5.unsqueeze(0).permute(3,0,1,2)\n",
        "image_6 = image_6.unsqueeze(0).permute(3,0,1,2)\n",
        "image_7 = image_7.unsqueeze(0).permute(3,0,1,2)\n",
        "\n",
        "label_1 = label_1.permute(2,0,1)\n",
        "label_2 = label_2.permute(2,0,1)\n",
        "label_3 = label_3.permute(2,0,1)\n",
        "label_4 = label_4.permute(2,0,1)\n",
        "label_5 = label_5.permute(2,0,1)\n",
        "label_6 = label_6.permute(2,0,1)\n",
        "label_7 = label_7.permute(2,0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBtS9ZAKGlmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514c6417-7ec5-4e52-f22a-8894ccd4e289"
      },
      "source": [
        "transformed_train_dataset_1 = KneeDataset(image_1, label_1,\n",
        "                                        transform=(None))\n",
        "transformed_train_dataset_2 = KneeDataset(image_2, label_2,\n",
        "                                        transform =(None))\n",
        "transformed_train_dataset_3 = KneeDataset(image_3, label_3,\n",
        "                                        transform =(None))\n",
        "transformed_train_dataset_4 = KneeDataset(image_4, label_4,\n",
        "                                       transform=(None))\n",
        "transformed_train_dataset_5 = KneeDataset(image_5, label_5,\n",
        "                                       transform=(None))\n",
        "transformed_train_dataset_6 = KneeDataset(image_6, label_6,\n",
        "                                        transform =(None))\n",
        "transformed_train_dataset_7 = KneeDataset(image_7, label_7,\n",
        "                                        transform =(None))\n",
        "transformed_train_dataset = torch.utils.data.ConcatDataset([transformed_train_dataset_1, transformed_train_dataset_2,transformed_train_dataset_3,\n",
        "                                                            transformed_train_dataset_4, transformed_train_dataset_5,\n",
        "                                                            transformed_train_dataset_6,transformed_train_dataset_7])\n",
        "#transformed_valid_dataset = torch.utils.data.ConcatDataset([transformed_valid_dataset_1, transformed_valid_dataset_2])\n",
        "\n",
        "image_datasets = {'train':transformed_train_dataset,\n",
        "                 # 'valid':transformed_valid_dataset,\n",
        "                  }\n",
        "dataloaders = {'train' : torch.utils.data.DataLoader(image_datasets['train'], \n",
        "                                                        batch_size=20, shuffle = True, num_workers = 4),\n",
        "                  #'valid': torch.utils.data.DataLoader(image_datasets['valid'], batch_size=20,\n",
        "                                             #shuffle=True, num_workers=4),\n",
        "                  \n",
        "                  }\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train']}\n",
        "print(dataset_sizes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train': 945}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWex61AUGmwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "32f5c293-f72a-4602-ba61-2c8cb166e235"
      },
      "source": [
        "batch= next(iter(dataloaders['train'])) #train\n",
        "inputs = batch['image']\n",
        "label = batch['label']\n",
        "print(inputs.shape)\n",
        "print(label.shape)\n",
        "\n",
        "'''\n",
        "batch_valid = next(iter(dataloaders['valid'])) #validation\n",
        "inputs = batch['image']\n",
        "label = batch['label']\n",
        "print(inputs.shape)\n",
        "print(label.shape)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 1, 217, 271])\n",
            "torch.Size([20, 217, 271])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nbatch_valid = next(iter(dataloaders['valid'])) #validation\\ninputs = batch['image']\\nlabel = batch['label']\\nprint(inputs.shape)\\nprint(label.shape)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o36eBz8eGosP",
        "outputId": "6118a7ad-ec14-422e-bb69-d6e72493fd67"
      },
      "source": [
        "print(torch.unique(batch['label']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxz0jRRvGqU2",
        "outputId": "3ff8f9f1-37a3-46cb-9f8c-f84c9fc02392"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda() ## Loss\n",
        "net = UNet_2d()\n",
        "net.apply(init_weights)\n",
        "net.cuda()\n",
        "net.train()\n",
        "total_epocs_var = 500 #500\n",
        "\n",
        "print(countParameters(net))\n",
        "\n",
        "optimizer = optim.Adam(list(net.parameters()),lr=0.002)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31042434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gueSZ-ggTvY1"
      },
      "source": [
        "#from torch.optim.lr_scheduler import StepLR\n",
        "#scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdhilkijGurt",
        "outputId": "40ffbaf7-34ae-466e-dd0d-fb9711bfdb1d"
      },
      "source": [
        "train_loss = torch.zeros(total_epocs_var)/0\n",
        "\n",
        "dice1 = []  #torch.zeros(500)/0\n",
        "dice2 = 0\n",
        "dice3 =[]\n",
        "\n",
        "for epoch_i in range(total_epocs_var):\n",
        "    net.train()\n",
        "    train_loss[epoch_i] = 0.0\n",
        "    batch_dice_t = 0\n",
        "    batch_dice_mean_t = 0\n",
        "    i=0\n",
        "    \n",
        "           \n",
        "      ####MODEL EXPERIMENT####\n",
        "    for i, batch in enumerate(dataloaders['train']):\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch['image'].float().cuda()\n",
        "        label = batch['label'].long().cuda()\n",
        "        inputs,label = augmentAffine(inputs,label,0.075)      \n",
        "        output = net(inputs)\n",
        "        loss = criterion(output,label)\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "        train_loss[epoch_i] += loss.item()\n",
        "        d1 = dice(output.argmax(1), label,2)\n",
        "        batch_dice_t += d1\n",
        "        i = i +1\n",
        "        batch_dice_mean_t += d1.mean().cpu()\n",
        "\n",
        "    train_loss[epoch_i] /= i\n",
        "    batch_dice_t/=i\n",
        "    batch_dice_mean_t/=i\n",
        "    dice2+=batch_dice_t   \n",
        "    dice1.append(batch_dice_t)\n",
        "    dice3.append(batch_dice_mean_t)\n",
        "        \n",
        "    if(epoch_i%25==5):\n",
        "            #print('Run Dice 2 All Labels Average',run_dice2/epoch_i)\n",
        "            print('epoch',epoch_i,'train loss',train_loss[epoch_i], 'dice train',dice3[epoch_i])\n",
        "            print('run_dice[epoch] Average',torch.tensor(dice3).float().mean())\n",
        "            \n",
        "        \n",
        "            \n",
        "print('Run Dice 2 All Labels Average Final',dice2/total_epocs_var)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3448: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 train loss tensor(0.0402) dice train tensor(0.9500)\n",
            "run_dice[epoch] Average tensor(0.8937)\n",
            "epoch 30 train loss tensor(0.0172) dice train tensor(0.9760)\n",
            "run_dice[epoch] Average tensor(0.9523)\n",
            "epoch 55 train loss tensor(0.0133) dice train tensor(0.9809)\n",
            "run_dice[epoch] Average tensor(0.9640)\n",
            "epoch 80 train loss tensor(0.0127) dice train tensor(0.9820)\n",
            "run_dice[epoch] Average tensor(0.9695)\n",
            "epoch 105 train loss tensor(0.0186) dice train tensor(0.9744)\n",
            "run_dice[epoch] Average tensor(0.9719)\n",
            "epoch 130 train loss tensor(0.0105) dice train tensor(0.9847)\n",
            "run_dice[epoch] Average tensor(0.9740)\n",
            "epoch 155 train loss tensor(0.0215) dice train tensor(0.9716)\n",
            "run_dice[epoch] Average tensor(0.9754)\n",
            "epoch 180 train loss tensor(0.0094) dice train tensor(0.9862)\n",
            "run_dice[epoch] Average tensor(0.9767)\n",
            "epoch 205 train loss tensor(0.0093) dice train tensor(0.9862)\n",
            "run_dice[epoch] Average tensor(0.9777)\n",
            "epoch 230 train loss tensor(0.0092) dice train tensor(0.9864)\n",
            "run_dice[epoch] Average tensor(0.9786)\n",
            "epoch 255 train loss tensor(0.0091) dice train tensor(0.9865)\n",
            "run_dice[epoch] Average tensor(0.9794)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxJhdYT6OwIK"
      },
      "source": [
        "  #Saving the model after training \n",
        "torch.save(net.train().cpu(),'/content/drive/MyDrive/training_at_500_inpat_bs20.')\n",
        "#torch.save(net.eval().cpu().state_dict(),'/content/drive/My Drive/train_300') #state_dict not required \n",
        "  # change the path depending on where you want to save in drive "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mu3m6n5PBWJ"
      },
      "source": [
        "# graphs for loss\n",
        "epoc=range(total_epocs_var)\n",
        "plt.plot(epoc,train_loss.cpu().numpy(),label='train_loss')\n",
        "plt.ylim(0,0.05)\n",
        "plt.legend()\n",
        "plt.title('Train Loss')\n",
        "plt.figure()\n",
        "plt.show()\n",
        "'''\n",
        "plt.plot(epoc,val_loss.cpu().numpy(),'r',label='valid_loss')\n",
        "#plt.ylim(0,0.05)\n",
        "plt.legend()\n",
        "plt.title('Valid Loss')\n",
        "plt.figure()\n",
        "plt.show()\n",
        "'''\n",
        "plt.plot(epoc,dice3,label='Train Dice')\n",
        "#plt.plot(epoc,val_l,label='Val_loss')\n",
        "plt.legend()\n",
        "plt.title('Train Dice Loss')\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "plt.plot(epoc,run_dice3,'g',label='Valid Dice')\n",
        "#plt.plot(epoc,val_l,label='Val_loss')\n",
        "#plt.ylim(0.3,1)\n",
        "plt.legend()\n",
        "plt.title('Valid Dice Loss')\n",
        "plt.figure()\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6bWdT0oVc91"
      },
      "source": [
        " if(True):\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss[epoch_i] = 0.0\n",
        "            batch_dice = 0\n",
        "            batch_dice_mean = 0\n",
        "            i=0\n",
        "            \n",
        "            for i, batch in enumerate(dataloaders['valid']):\n",
        "            \n",
        "                \n",
        "                #optimizer.zero_grad()\n",
        "                inputs_v = batch['image'].float().cuda()\n",
        "                label_v = batch['label'].long().cuda()\n",
        "                inputs_v,label_v = augmentAffine(inputs_v,label_v,0.075)\n",
        "                output_v = net(inputs_v)\n",
        "                loss = criterion(output_v,label_v)\n",
        "                val_loss[epoch_i] += loss.item()\n",
        "              ##  scheduler.step()\n",
        "                \n",
        "            \n",
        "                d1 = dice(output_v.argmax(1), label_v, 2)\n",
        "                \n",
        "                batch_dice += d1\n",
        "                i = i+1\n",
        "                batch_dice_mean += d1.mean().cpu()\n",
        "              \n",
        "            val_loss[epoch_i]/=i\n",
        "            batch_dice/=i\n",
        "            batch_dice_mean/=i\n",
        "            \n",
        "            run_dice2+=batch_dice\n",
        "            run_dice.append(batch_dice)\n",
        "            run_dice3.append(batch_dice_mean)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}